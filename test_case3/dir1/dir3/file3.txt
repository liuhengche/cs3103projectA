A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.
A real-time (RT) system consists of three parts: the hardware, the operating system, and the application. All three parts must meet the latency deadline for the whole system to correctly function. I have covered how to deal with the hardware and the operating system (OS) in the previous post. The natural next question is: how do we ensure the application behaves correctly under RT constraints? The answer to this is surprising complicated and requires a few posts to cover the basics. In this post, I will discuss the sources of latency coming from the application. From these, best practices for writing an RT application can be established.
In the last post, I did promise to write some code. This unfortunately must be delayed for one last time, as this post is too long as is.
RT vs non-RT sections of the code
Before I begin, it is important to clarify that an RT application has two sections in its code: a non-RT section and a RT section. Code in the RT section must execute within the required deadline while code executing in the non-RT section do not have a (strict) deadline. The program flow of two typical RT applications are shown in Figure 1 below. When the program is initializing or shutting down, it is usually in the non-RT section. During initialization, multiple threads are typically created. Usually there are at least two threads: an RT one and a non-RT one. In the robot application example (left), there are two threads: a non-RT data logger thread and an RT PID controller thread. The initialization and shutdown sections of the RT PID controller thread are also not RT. Only the actual PID controller loop is. A similar architecture exists for the audio production program (right), and basically every other multi-threaded RT application. Most of the time, we only need to worry about sources of latency in the RT sections of the program. Thus, the practices discussed in this post are only necessary in the RT sections and usually unnecessary in the non-RT sections.
Avoid non-constant worst-case execution time
It turns out, there is a single rule that you need to follow to be successful at real-time programming: do not call code in the RT section if you don't know its worst-case execution time. Basically, this is saying that you cannot use non-constant time algorithms, unless the input size is low enough that the actual execution time of the algorithm remains acceptably bounded. Unfortunately, most algorithmic implementations favour throughput over latency, which usually means they are tailored to the average-case execution time. Worst-case execution time could be orders of magnitude worse than the average case and such occasions could occur infrequently (e.g. std::vector.push_back). The worst-case execution times also tend to be under-documented (if at all). If the RT system cannot tolerate any deadline misses, specialized libraries and algorithms are usually needed. However, most RT applications do not need such hard guarantees. In these situations, "normal" libraries such as the C++ standard library can be safely used if the worst-case execution time can be acceptably determined. There is a talk in 2021 by Timur Doumler that presents ways to avoid function calls with unbounded latency in the C++ standard library that I highly recommend for anyone working on RT C++ applications.
However, the implementation of algorithms is not the only type of latency. An additional source of latency exists in the interaction between the application and the OS. Such latency cannot be inferred from reading the application code by itself. As noted in part 1, programming on Linux and other general-purpose operating systems is almost like magic. Although the operating system is juggling between thousands of concurrent tasks with a limited amount of CPU and memory, the application can be written as if it is the only process consuming CPU and memory. The way this is accomplished on Linux (and other general-purpose OSes) is optimized for throughput instead of the worst-case latency. Since RT applications need to ensure that worst-case performance remain acceptable, we must understand the OS-level magic to ensure the application won't miss its deadline. The two main kernel subsystems that we need to understand and cooperate with are: (1) the virtual memory system and (2) the CPU scheduler.